{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Dataset\n",
    "def load_dataset(file_path):\n",
    "    logging.info(\"Loading dataset...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    logging.info(f\"Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning\n",
    "def clean_data(data):\n",
    "    logging.info(\"Cleaning data...\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Check for and handle missing values\n",
    "    logging.info(\"Missing values before cleaning:\")\n",
    "    logging.info(data.isnull().sum())\n",
    "    \n",
    "    # Remove outliers using IQR method\n",
    "    def remove_outliers(df, columns):\n",
    "        for col in columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        return df\n",
    "    \n",
    "    numerical_columns = ['TV', 'Radio', 'Newspaper', 'Sales']\n",
    "    data = remove_outliers(data, numerical_columns)\n",
    "    \n",
    "    logging.info(f\"Data cleaned. Remaining rows: {data.shape[0]}.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "def perform_eda(data):\n",
    "    logging.info(\"Performing EDA...\")\n",
    "    \n",
    "    # Correlation Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Pairplot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.pairplot(data, diag_kind='kde')\n",
    "    plt.suptitle('Pairwise Relationships', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pairplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Scatter plots of features vs Sales\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, col in enumerate(['TV', 'Radio', 'Newspaper'], 1):\n",
    "        plt.subplot(1, 3, i)\n",
    "        sns.scatterplot(x=col, y='Sales', data=data)\n",
    "        plt.title(f'{col} vs Sales')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_vs_sales.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print correlation with Sales\n",
    "    logging.info(\"\\nCorrelation with Sales:\")\n",
    "    logging.info(data.corr()['Sales'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Engineering\n",
    "def feature_engineering(data):\n",
    "    logging.info(\"Performing feature engineering...\")\n",
    "    \n",
    "    X = data.drop('Sales', axis=1)\n",
    "    y = data['Sales']\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Add polynomial features\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X_scaled)\n",
    "    \n",
    "    return X_poly, y, scaler, poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Model Training and Evaluation\n",
    "def train_and_evaluate_models(X, y):\n",
    "    logging.info(\"Splitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    logging.info(\"Training models...\")\n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'Linear Regression': LinearRegression()\n",
    "    }\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions[name] = y_pred\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        logging.info(f\"{name} R2 Score: {r2:.2f}\")\n",
    "    \n",
    "    # Hyperparameter Tuning for Random Forest\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), \n",
    "                                  param_grid_rf, cv=3, scoring='r2', n_jobs=-1)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_rf = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Evaluate Best Model\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    logging.info(\"Evaluating best model...\")\n",
    "    print(\"\\nBest Model Evaluation:\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"R-Squared Score: {r2:.2f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "    \n",
    "    # Residual Plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=30, color='blue')\n",
    "    plt.title('Residuals Distribution')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('residuals_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Actual vs Predicted Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for name, y_pred in predictions.items():\n",
    "        plt.scatter(y_test, y_pred, alpha=0.6, label=name)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='red', label='Perfect Prediction')\n",
    "    plt.title('Actual vs Predicted Sales for All Models')\n",
    "    plt.xlabel('Actual Sales')\n",
    "    plt.ylabel('Predicted Sales')\n",
    "    plt.legend()\n",
    "    plt.savefig('actual_vs_predicted.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return best_rf, grid_search_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 21:45:25,106 - Loading dataset...\n",
      "2024-12-20 21:45:25,128 - Dataset loaded with 200 rows and 4 columns.\n",
      "2024-12-20 21:45:25,129 - Cleaning data...\n",
      "2024-12-20 21:45:25,131 - Missing values before cleaning:\n",
      "2024-12-20 21:45:25,132 - TV           0\n",
      "Radio        0\n",
      "Newspaper    0\n",
      "Sales        0\n",
      "dtype: int64\n",
      "2024-12-20 21:45:25,137 - Data cleaned. Remaining rows: 198.\n",
      "2024-12-20 21:45:25,137 - Performing EDA...\n",
      "2024-12-20 21:45:27,792 - \n",
      "Correlation with Sales:\n",
      "2024-12-20 21:45:27,792 - Sales        1.000000\n",
      "TV           0.899974\n",
      "Radio        0.348566\n",
      "Newspaper    0.151764\n",
      "Name: Sales, dtype: float64\n",
      "2024-12-20 21:45:27,792 - Performing feature engineering...\n",
      "2024-12-20 21:45:27,801 - Splitting data...\n",
      "2024-12-20 21:45:27,801 - Training models...\n",
      "2024-12-20 21:45:27,927 - Random Forest R2 Score: 0.96\n",
      "2024-12-20 21:45:27,986 - Gradient Boosting R2 Score: 0.95\n",
      "2024-12-20 21:45:27,994 - Linear Regression R2 Score: 0.93\n",
      "2024-12-20 21:45:32,812 - Evaluating best model...\n",
      "C:\\Users\\ranaa\\AppData\\Local\\Temp\\ipykernel_17184\\538049114.py:61: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k--\" (-> color='k'). The keyword argument will take precedence.\n",
      "  plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', color='red', label='Perfect Prediction')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model Evaluation:\n",
      "Mean Squared Error: 1.14\n",
      "R-Squared Score: 0.96\n",
      "Mean Absolute Error: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 21:45:33,141 - Saving the best model...\n",
      "2024-12-20 21:45:33,187 - Best Hyperparameters:\n",
      "2024-12-20 21:45:33,187 - Model saved as sales_prediction_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main Execution\n",
    "def main(file_path=r'F:\\CodSoft\\SalesPrediction\\advertising.csv'):\n",
    "    # Full Pipeline\n",
    "    data = load_dataset(file_path)\n",
    "    cleaned_data = clean_data(data)\n",
    "    perform_eda(cleaned_data)\n",
    "    \n",
    "    X, y, scaler, poly = feature_engineering(cleaned_data)\n",
    "    best_model, best_params = train_and_evaluate_models(X, y)\n",
    "    \n",
    "    logging.info(\"Saving the best model...\")\n",
    "    joblib.dump({\n",
    "        'model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'poly_features': poly\n",
    "    }, 'sales_prediction_model.pkl')\n",
    "    \n",
    "    logging.info(\"Best Hyperparameters:\")\n",
    "    print(best_params)\n",
    "    logging.info(\"Model saved as sales_prediction_model.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
